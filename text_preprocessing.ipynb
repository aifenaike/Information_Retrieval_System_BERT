{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages and modules\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing_utils import *\n",
    "\n",
    "# reading the contractions data from text file\n",
    "with open('contractions.txt') as f:\n",
    "    contractions = f.read()\n",
    "print(\"Data type before reconstruction : \", type(contractions))\n",
    "      \n",
    "# reconstructing the data as a dictionary\n",
    "contractions_dict = ast.literal_eval(contractions)\n",
    "  \n",
    "print(\"Data type after reconstruction : \", type(contractions_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training and testing queries.\n",
    "train_queries = pd.read_csv(\"train_queries.csv\")\n",
    "test_queries = pd.read_csv(\"test_queries.csv\")\n",
    "\n",
    "#load training and testing corpus.\n",
    "train_corpus = pd.read_csv(\"training_corpus.csv\")\n",
    "test_corpus = pd.read_csv(\"testing_corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7240ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "train_corpus['cleaned'] = train_corpus['body'].apply(lambda x:x.lower())\n",
    "test_corpus['cleaned'] = test_corpus['body'].apply(lambda x:x.lower())\n",
    "\n",
    "# Expanding Contractions\n",
    "train_corpus['cleaned']=train_corpus['cleaned'].apply(lambda x:expand_contractions(x))\n",
    "test_corpus['cleaned']=test_corpus['cleaned'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164580c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "train_corpus['cleaned']=train_corpus['cleaned'].apply(lambda x: clean_text(x))\n",
    "test_corpus['cleaned']=test_corpus['cleaned'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords removal & Lemmatizing tokens using SpaCy\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=['ner','parser'])\n",
    "nlp.max_length=5000000\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Removing Stopwords and Lemmatizing words\n",
    "train_corpus['lemmatized']=train_corpus['cleaned'].progress_apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
    "test_corpus['lemmatized']=test_corpus['cleaned'].progress_apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dc648",
   "metadata": {},
   "source": [
    "- **We have now pre-processed our documents. Itâ€™s time to pre-process our queries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "train_queries['cleaned']=train_queries['query'].apply(lambda x:x.lower())\n",
    "test_queries['cleaned']=test_queries['query'].apply(lambda x:x.lower())\n",
    "\n",
    "# Expanding contractions\n",
    "train_queries['cleaned']=train_queries['cleaned'].apply(lambda x:expand_contractions(x))\n",
    "test_queries['cleaned']=test_queries['cleaned'].apply(lambda x:expand_contractions(x))\n",
    "\n",
    "# Cleaning queries using RegEx\n",
    "train_queries['cleaned']=train_queries['cleaned'].apply(lambda x: clean_text(x))\n",
    "test_queries['cleaned']=test_queries['cleaned'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Removing extra spaces\n",
    "train_queries['cleaned']=train_queries['cleaned'].apply(lambda x: re.sub(' +',' ',x))\n",
    "test_queries['cleaned']=test_queries['cleaned'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining corpus and queries for training\n",
    "combined_training=pd.concat([train_corpus.rename(columns={'lemmatized':'text'})['text'],\\\n",
    "                             train_queries.rename(columns={'cleaned':'text'})['text']])\\\n",
    "                             .sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b0f9a",
   "metadata": {},
   "source": [
    "- We will train a word2vec model and generate vectors for documents and queries in the testing set for information retrieval. But before that, there is a need to prepare the dataset for training the word2vec model. \n",
    "- Please note, we have already created the training set, but we want to use the same word2vec model for generating vectors for both documents and queries. Thus, we will combine both documents and queries to create a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#Creating data for the model training\n",
    "train_data=[]\n",
    "for i in combined_training:\n",
    "    train_data.append(i.split())\n",
    "\n",
    "# Training a word2vec model from the train data set.\n",
    "w2v_model = Word2Vec(train_data, vector_size=300, min_count=2,window=5, sg=1,workers=4)\n",
    "w2v_model.save(\"models/word2vec.model\")\n",
    "\n",
    "# Vocabulary size\n",
    "print('Vocabulary size:', len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Getting Word2Vec Vectors for Testing Corpus.\n",
    "test_corpus['embeddings']=test_corpus['lemmatized'].progress_apply(lambda x :aggregate_embedding_w2v(w2v_model,x.split()))\n",
    "# Getting Word2Vec Vectors for Training Corpus.\n",
    "train_corpus['embeddings']=train_corpus['lemmatized'].progress_apply(lambda x :aggregate_embedding_w2v(w2v_model,x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a174cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the embeddings of the documents in test set.\n",
    "test_embeddings = np.concatenate(test_corpus.embeddings.values,axis=0)\n",
    "np.save('data/word2vec/test_embeddings.npy',test_embeddings)\n",
    "\n",
    "#save the embeddings of the documents in train set.\n",
    "train_embeddings = np.concatenate(train_corpus.embeddings.values,axis=0)\n",
    "np.save('data/word2vec/train_embeddings.npy',train_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:faiss_env] *",
   "language": "python",
   "name": "conda-env-faiss_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
